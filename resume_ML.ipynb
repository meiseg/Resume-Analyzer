{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libaries\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Category                                             Resume\n",
      "0  Frontend Developer  As a seasoned Frontend Developer, I have a pro...\n",
      "1   Backend Developer  With a solid background in Backend Development...\n",
      "2    Python Developer  As a Python Developer, I leverage my expertise...\n",
      "3      Data Scientist  With a background in Data Science, I possess a...\n",
      "4  Frontend Developer  Experienced Frontend Developer with a passion ...\n",
      "Category    0\n",
      "Resume      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"gpt_dataset.csv\")\n",
    "\n",
    "\n",
    "df.dropna(subset=[\"Category\",\"Resume\"],inplace=True)\n",
    "\n",
    "print(df.head())\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "df[\"processed_resume\"] = df[\"Resume\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract keywords from text using SpaCy\n",
    "def extract_keywords(text, top_n=10):\n",
    "    \"\"\"\n",
    "    Extract top_n keywords from the text using SpaCy.\n",
    "    Keywords are nouns, proper nouns, and adjectives.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    keywords = [\n",
    "        token.text.lower() for token in doc\n",
    "        if token.is_alpha and not token.is_stop and token.pos_ in [\"NOUN\", \"PROPN\", \"ADJ\"]\n",
    "    ]\n",
    "    keyword_counts = Counter(keywords)\n",
    "    return [word for word, _ in keyword_counts.most_common(top_n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_category_keywords(df, column=\"resume\", category_column=\"category\", top_n=10):\n",
    "    \"\"\"\n",
    "    Extract top_n keywords for each category in the dataset.\n",
    "    \"\"\"\n",
    "    category_keywords = defaultdict(list)\n",
    "    \n",
    "    # Group resumes by category\n",
    "    grouped = df.groupby(category_column)[column].apply(\" \".join)\n",
    "    \n",
    "    for category, resumes in grouped.items():\n",
    "        keywords = extract_keywords(resumes, top_n=top_n)\n",
    "        category_keywords[category] = keywords\n",
    "    \n",
    "    return category_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_keywords = extract_category_keywords(df, column=\"Resume\", category_column=\"Category\", top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Category  category_encoded\n",
      "0  Frontend Developer                 3\n",
      "1   Backend Developer                 0\n",
      "2    Python Developer                 7\n",
      "3      Data Scientist                 2\n",
      "4  Frontend Developer                 3\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the 'category' column\n",
    "df[\"category_encoded\"] = label_encoder.fit_transform(df[\"Category\"])\n",
    "\n",
    "# Inspect the encoded labels\n",
    "print(df[[\"Category\", \"category_encoded\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 1015)\n"
     ]
    }
   ],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit to 5000 features for simplicity\n",
    "\n",
    "# Fit and transform the processed text data\n",
    "X = vectorizer.fit_transform(df[\"processed_resume\"])\n",
    "\n",
    "# Convert to a dense array (if needed)\n",
    "X = X.toarray()\n",
    "\n",
    "# Inspect the shape of the feature matrix\n",
    "print(X.shape)  # (num_samples, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract encoded labels\n",
    "y = df[\"category_encoded\"].values\n",
    "\n",
    "# Convert labels to PyTorch tensor\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)  # Use torch.long for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 1015])\n",
      "torch.Size([400])\n"
     ]
    }
   ],
   "source": [
    "# Convert features to PyTorch tensor\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "# Inspect the tensors\n",
    "print(X_tensor.shape)  # (num_samples, num_features)\n",
    "print(y_tensor.shape)  # (num_samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([320, 1015]) torch.Size([80, 1015])\n",
      "torch.Size([320]) torch.Size([80])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Inspect the shapes\n",
    "print(X_train.shape, X_test.shape)  # Training and testing feature sets\n",
    "print(y_train.shape, y_test.shape)  # Training and testing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "class ResumeClassifier(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_classes):\n",
    "        super(ResumeClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Input layer to hidden layer\n",
    "        self.relu = nn.ReLU()                          # Activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # Hidden layer to output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]  # Number of features (5000 from TF-IDF)\n",
    "hidden_size = 128              # Size of the hidden layer\n",
    "num_classes = len(df[\"category_encoded\"].unique())  # Number of unique categories\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResumeClassifier(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.0561\n",
      "Epoch [2/10], Loss: 1.9496\n",
      "Epoch [3/10], Loss: 1.7882\n",
      "Epoch [4/10], Loss: 1.5755\n",
      "Epoch [5/10], Loss: 1.3195\n",
      "Epoch [6/10], Loss: 1.0447\n",
      "Epoch [7/10], Loss: 0.7827\n",
      "Epoch [8/10], Loss: 0.5605\n",
      "Epoch [9/10], Loss: 0.3948\n",
      "Epoch [10/10], Loss: 0.2796\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.50%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_resume(resume_text, model, vectorizer, label_encoder):\n",
    "    # Preprocess the resume text\n",
    "    processed_text = preprocess_text(resume_text)\n",
    "    \n",
    "    # Vectorize the resume text\n",
    "    resume_vector = vectorizer.transform([processed_text]).toarray()\n",
    "    resume_tensor = torch.tensor(resume_vector, dtype=torch.float32)\n",
    "    \n",
    "    # Predict the category\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(resume_tensor)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        confidence, predicted_class = torch.max(probabilities, 1)\n",
    "    \n",
    "    # Decode the predicted category\n",
    "    predicted_category = label_encoder.inverse_transform(predicted_class.numpy())[0]\n",
    "    return predicted_category, confidence.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_resume_quality(resume_text, predicted_category, category_keywords):\n",
    "    \"\"\"\n",
    "    Calculate a quality score for the resume based on multiple criteria.\n",
    "    \"\"\"\n",
    "    # Preprocess the resume text\n",
    "    processed_text = preprocess_text(resume_text)\n",
    "    \n",
    "    # Keyword coverage: Check for relevant keywords in the resume\n",
    "    keywords = category_keywords.get(predicted_category, [])\n",
    "    keyword_coverage = sum(1 for keyword in keywords if keyword in processed_text.lower()) / len(keywords) if keywords else 0\n",
    "    \n",
    "    # Completeness: Check for essential sections (e.g., \"experience\", \"education\", \"skills\")\n",
    "    essential_sections = [\"experience\", \"education\", \"skills\"]\n",
    "    completeness_score = sum(1 for section in essential_sections if section in processed_text.lower()) / len(essential_sections)\n",
    "    \n",
    "    # Combine scores into a final quality score\n",
    "    quality_score = (\n",
    "        0.6 * keyword_coverage +       # Weight for keyword coverage\n",
    "        0.4 * completeness_score       # Weight for completeness\n",
    "    )\n",
    "    \n",
    "    return quality_score, keyword_coverage, completeness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document  # Library to extract text from DOCX files\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    \"\"\"Extract text from a .docx file.\"\"\"\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "    return \"\\n\".join(full_text)\n",
    "\n",
    "def score_resume_from_docx(file_path, model, vectorizer, label_encoder, category_keywords):\n",
    "    # Extract text from the .docx file\n",
    "    resume_text = extract_text_from_docx(file_path)\n",
    "    \n",
    "    # Preprocess the resume text\n",
    "    processed_text = preprocess_text(resume_text)\n",
    "    \n",
    "    # Vectorize the resume text\n",
    "    resume_vector = vectorizer.transform([processed_text]).toarray()\n",
    "    resume_tensor = torch.tensor(resume_vector, dtype=torch.float32)\n",
    "    \n",
    "    # Predict the category\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(resume_tensor)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        confidence, predicted_class = torch.max(probabilities, 1)\n",
    "    \n",
    "    # Decode the predicted category\n",
    "    predicted_category = label_encoder.inverse_transform(predicted_class.numpy())[0]\n",
    "    \n",
    "    # Calculate the quality score\n",
    "    quality_score, keyword_coverage, completeness_score = calculate_resume_quality(\n",
    "        resume_text, predicted_category, category_keywords\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"predicted_category\": predicted_category,\n",
    "        \"confidence_score\": confidence.item(),\n",
    "        \"quality_score\": quality_score,\n",
    "        \"keyword_coverage\": keyword_coverage,\n",
    "        \"completeness_score\": completeness_score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category: Data Scientist\n",
      "Confidence Score: 0.3263\n",
      "Quality Score: 0.5667\n",
      "Keyword Coverage: 0.5000\n",
      "Completeness Score: 0.6667\n"
     ]
    }
   ],
   "source": [
    "docx_resume_path = \"Melvin_Escobar_resume.docx\"  # Replace with the path to your DOCX resume\n",
    "# Score the resume\n",
    "# Score the resume\n",
    "results = score_resume_from_docx(docx_resume_path, model, vectorizer, label_encoder, category_keywords)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Predicted Category: {results['predicted_category']}\")\n",
    "print(f\"Confidence Score: {results['confidence_score']:.4f}\")\n",
    "print(f\"Quality Score: {results['quality_score']:.4f}\")\n",
    "print(f\"Keyword Coverage: {results['keyword_coverage']:.4f}\")\n",
    "print(f\"Completeness Score: {results['completeness_score']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
